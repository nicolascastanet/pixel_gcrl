defaults:
  - method: default
  - _self_

save_episode: True

seed: 0
plot_goals: False
rl_algo: TQC

env:
  max_episode_steps: 50
  num_envs: 20
  maze_type: 'square_pbcs_0'
  xml_files: None
  num_eval_rollout: 500
  change_env_steps_list: None
  from_pixel: False
  cnn_policy: False
  latent_dim_obs: 2
  dist_thresh_latent: 0.15
  reward: 'sparse'

vae:
  state_dict_path: None
  pretrain: False
  learning_rate: 1e-3
  beta: 1

actor:
  gamma: 0.98
  actor_lr: 1e-3
  actor_layers: [512,512,512]
  critic_lr: 1e-3
  critic_layers: [512,512,512]
  tau: 5e-2
  target_update_period: 40
  buffer_size: 1_000_000
  batch_size: 2000
  gd_steps_per_step: 1
  max_steps: 4_000_000
  evaluate_every_x_steps: 5_000
  save_agent_every_x_steps: 100_000
  

  